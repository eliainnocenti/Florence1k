{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["ZZnH7GOxj2pf"],"authorship_tag":"ABX9TyN6b/TiAHqq04KmDj0nX3Aj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"Df_8E2QYicBf"}},{"cell_type":"code","source":["!python --version\n","!pip install --upgrade pip\n","!pip install mediapipe-model-maker"],"metadata":{"id":"lnnF7WjKixrb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","import os\n","import json\n","from tqdm import tqdm\n","import tensorflow as tf\n","assert tf.__version__.startswith('2')\n","\n","from mediapipe_model_maker import object_detector\n","\n","from google.colab import drive\n","import shutil\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"],"metadata":{"id":"gVhHW6Brj7a4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Colab Pro"],"metadata":{"id":"O53RNiLqijD5"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"id":"pHidcNdgiwMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"gPvEeEiuj_tL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Set paths"],"metadata":{"id":"tC5Z-4EuiqfK"}},{"cell_type":"code","source":["# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"id":"FFWPW6iOiw3T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_path = '/content/drive/MyDrive/'\n","source_path = base_path + 'Datasets/florence1k/'\n","\n","dest_base_path = base_path + 'MyProject/florence1k/'\n","\n","train_dataset_path = dest_base_path + 'train/'\n","validation_dataset_path = dest_base_path + 'validation/'\n","test_dataset_path = dest_base_path + 'test/'"],"metadata":{"id":"5wfXEsoykD1Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Create directories\n","\n","os.makedirs(dest_base_path, exist_ok=True)\n","\n","os.makedirs(train_dataset_path, exist_ok=True)\n","os.makedirs(validation_dataset_path, exist_ok=True)\n","os.makedirs(test_dataset_path, exist_ok=True)\n","\n","os.makedirs(os.path.join(train_dataset_path, 'images'), exist_ok=True)\n","os.makedirs(os.path.join(validation_dataset_path, 'images'), exist_ok=True)\n","os.makedirs(os.path.join(test_dataset_path, 'images'), exist_ok=True)"],"metadata":{"id":"usclWcg2kJSf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare Data"],"metadata":{"id":"4JHzi79Zituc"}},{"cell_type":"markdown","source":["### Copy images"],"metadata":{"id":"wKn-N6lNivLM"}},{"cell_type":"code","source":["# Function to copy images\n","def copy_images(file_list, dest_folder):\n","    with open(file_list, 'r') as f:\n","        lines = f.readlines()\n","        for line in tqdm(lines, desc=f\"Copying images to {dest_folder}\"):\n","            img_name = line.strip()\n","            src = os.path.join(source_path, img_name)\n","            dst = os.path.join(dest_folder, img_name)\n","            os.makedirs(os.path.dirname(dst), exist_ok=True)\n","            shutil.copy2(src, dst)\n","\n","# Copy images for each set\n","if os.listdir(train_dataset_path + 'images/') == [] and \\\n","   os.listdir(validation_dataset_path + 'images/') == [] and \\\n","   os.listdir(test_dataset_path + 'images/') == []:\n","    copy_images(dest_base_path + 'train.txt', train_dataset_path + 'images/')\n","    copy_images(dest_base_path + 'val.txt', validation_dataset_path + 'images/')\n","    copy_images(dest_base_path + 'test.txt', test_dataset_path + 'images/')\n","    print(\"Dataset division completed!\\n\")\n","else:\n","    print(\"One or more directories are not empty. Copy operation aborted.\\n\")\n","\n","print(f\"Number of images in train set: {len(os.listdir(train_dataset_path + 'images/'))}\")\n","print(f\"Number of images in validation set: {len(os.listdir(validation_dataset_path + 'images/'))}\")\n","print(f\"Number of images in test set: {len(os.listdir(test_dataset_path + 'images/'))}\")"],"metadata":{"id":"Mat8pS45i9ZG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Review Data"],"metadata":{"id":"jYktX1k_i_Ra"}},{"cell_type":"code","source":["with open(os.path.join(train_dataset_path, \"labels.json\"), \"r\") as f:\n","  labels_json = json.load(f)\n","for category_item in labels_json[\"categories\"]:\n","  print(f\"{category_item['id']}: {category_item['name']}\")"],"metadata":{"id":"tmJsdywDjBBu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create Dataset"],"metadata":{"id":"I-sDzWkKjCng"}},{"cell_type":"code","source":["# TODO: do I need this instruction ?\n","\n","cache_dirs = [\"/tmp/od_data/train\", \"/tmp/od_data/validation\"]\n","\n","for cache_dir in cache_dirs:\n","    if os.path.exists(cache_dir):\n","        shutil.rmtree(cache_dir)"],"metadata":{"id":"0GBBo20EjE95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = object_detector.Dataset.from_coco_folder(train_dataset_path, cache_dir=\"/tmp/od_data/train\")\n","validation_data = object_detector.Dataset.from_coco_folder(validation_dataset_path, cache_dir=\"/tmp/od_data/validation\")\n","\n","print(f\"{'Training Dataset Size:':<25} {train_data.size:>4}\")\n","print(f\"{'Validation Dataset Size:':<25} {validation_data.size:>4}\")"],"metadata":{"id":"NAZzgAlvimNN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Augmentation"],"metadata":{"id":"aFbT29GfjKn_"}},{"cell_type":"markdown","source":["### Augment Data"],"metadata":{"id":"OFdJzI9LjMHU"}},{"cell_type":"code","source":["import albumentations as A\n","import numpy as np\n","import cv2"],"metadata":{"id":"sMra1bIqjPEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_transform(set='train'):\n","    bboxes_params = A.BboxParams(format='coco', min_visibility=0.3, label_fields=['class_labels']) # TODO: check min_visibility\n","\n","    if set == 'train':\n","        transform = A.Compose([ # TODO: update pipeline (?)\n","            # TODO: do I need to resize images?\n","            #A.RandomResizedCrop(height=640, width=640, scale=(0.8, 1.0), ratio=(0.9, 1.1), p=1.0), # TODO: check h,w\n","            A.HorizontalFlip(p=0.5),\n","            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n","            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n","            A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n","            A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=0.3),\n","            A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.5),\n","            A.OneOf([\n","                A.MotionBlur(blur_limit=7, p=0.5),\n","                A.MedianBlur(blur_limit=7, p=0.5),\n","                A.GaussianBlur(blur_limit=7, p=0.5),\n","            ], p=0.3),\n","            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, border_mode=0, p=0.5),\n","            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # TODO: check\n","        ], bbox_params=bboxes_params)\n","\n","    elif set == 'validation':\n","        transform = A.Compose([ # TODO: update pipeline\n","            # TODO: do I need to resize images?\n","            A.HorizontalFlip(p=0.5),\n","            A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.5),\n","            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # TODO: check\n","        ], bbox_params=bboxes_params)\n","\n","    return transform"],"metadata":{"id":"-YDCi9QRi5V7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(os.path.join(train_dataset_path, 'labels.json'), 'r') as f:\n","    train_json = json.load(f)\n","\n","with open(os.path.join(validation_dataset_path, 'labels.json'), 'r') as f:\n","    val_json = json.load(f)\n","\n","with open(os.path.join(test_dataset_path, 'labels.json'), 'r') as f:\n","    test_json = json.load(f)\n","\n","n_images = max(train_json['images'][-1]['id'], val_json['images'][-1]['id'], test_json['images'][-1]['id'])\n","n_annotations = max(train_json['annotations'][-1]['id'], val_json['annotations'][-1]['id'], test_json['annotations'][-1]['id'])"],"metadata":{"id":"tUIZrKSvi8_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clip_bbox(bbox, image_width, image_height):\n","    x_min, y_min, width, height = bbox\n","\n","    x_min = max(0, min(x_min, image_width - 1)) # TODO: check -1\n","    y_min = max(0, min(y_min, image_height - 1)) # TODO: check -1\n","    width = min(width, image_width - x_min)\n","    height = min(height, image_height - y_min)\n","\n","    return [x_min, y_min, width, height]"],"metadata":{"id":"kcY6jUemi_fX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate_bbox(bbox, image_width, image_height):\n","    x, y, w, h = bbox\n","\n","    return 0 <= x < image_width and 0 <= y < image_height and x + w <= image_width and y + h <= image_height"],"metadata":{"id":"n2xKxBVijBj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def apply_augmentation(image_path, bboxes, class_labels, output_path, output_filename, transform):\n","    # Read the image\n","    image = cv2.imread(image_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image_height, image_width = image.shape[:2]\n","\n","    # Apply the augmentation\n","    try:\n","        transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n","    except Exception as e:\n","        print(f\"Error during transformation: {e}\")\n","        return [], []\n","\n","    # Save the augmented image\n","    augmented_image_path = os.path.join(output_path, output_filename)\n","    cv2.imwrite(augmented_image_path, cv2.cvtColor(transformed['image'], cv2.COLOR_RGB2BGR))\n","\n","    return transformed['bboxes'], transformed['class_labels']"],"metadata":{"id":"9E2TUqeHjESs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def augment_dataset(input_path, output_path, transform, n_images, n_annotations, num_augmentations=5):\n","    # Load the original COCO JSON file\n","    with open(os.path.join(input_path, 'labels.json'), 'r') as f:\n","        coco_data = json.load(f)\n","\n","    new_images = []\n","    new_annotations = []\n","\n","    # Copy original images and annotations\n","    for img in tqdm(coco_data['images'], desc=\"Copying original images\"):\n","\n","        src_path = os.path.join(input_path, 'images', img['file_name'])\n","        dst_path = os.path.join(output_path, 'images', img['file_name'])\n","        shutil.copy2(src_path, dst_path)\n","\n","        new_images.append(img)\n","        img_anns = [ann for ann in coco_data['annotations'] if ann['image_id'] == img['id']]\n","        new_annotations.extend(img_anns)\n","\n","    '''debug'''\n","    print(\"Before augmentation:\")\n","    print(f\"Number of images: {len(new_images)}\")\n","    print(f\"Number of annotations: {len(new_annotations)}\")\n","\n","    # Apply augmentations\n","    for img in tqdm(coco_data['images'], desc=\"Augmenting images\"):\n","        image_path = os.path.join(input_path, 'images', img['file_name'])\n","\n","        annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == img['id']]\n","\n","        image = cv2.imread(image_path)\n","        image_height, image_width = image.shape[:2]\n","\n","        for i in range(num_augmentations):\n","            bboxes = [ann['bbox'] for ann in annotations]\n","            class_labels = [ann['category_id'] for ann in annotations]\n","\n","            # TODO: should I call the function clip_bbox() regardless of the function validate_bbox()?\n","            for bbox in bboxes:\n","                if not validate_bbox(bbox, image_width, image_height):\n","                    bboxes = [clip_bbox(bbox, image_width, image_height) for bbox in bboxes]\n","\n","            new_filename = f\"{os.path.splitext(img['file_name'])[0]}_aug_{i}.jpg\"\n","\n","            new_bboxes, new_class_labels = apply_augmentation(\n","                image_path, bboxes, class_labels,\n","                os.path.join(output_path, 'images'), new_filename, transform\n","            )\n","\n","            new_img_id = n_images + 1\n","            new_images.append({\n","                'id': new_img_id,\n","                'file_name': new_filename\n","            })\n","\n","            n_images = n_images + 1\n","\n","            for bbox, cat_id in zip(new_bboxes, new_class_labels):\n","                new_annotations.append({\n","                    'id': n_annotations + 1,\n","                    'image_id': new_img_id,\n","                    'category_id': cat_id,\n","                    'bbox': bbox\n","                })\n","\n","                n_annotations = n_annotations + 1\n","\n","    '''debug'''\n","    print(\"After augmentation:\")\n","    print(f\"Number of images: {len(new_images)}\")\n","    print(f\"Number of annotations: {len(new_annotations)}\")\n","\n","    # Create the new COCO JSON file\n","    new_coco_data = {\n","        'categories': coco_data['categories'],\n","        'images': new_images,\n","        'annotations': new_annotations\n","    }\n","\n","    # Save the new COCO JSON file\n","    with open(os.path.join(output_path, 'labels.json'), 'w') as f:\n","        json.dump(new_coco_data, f, indent=4)\n","\n","    return n_images, n_annotations"],"metadata":{"id":"PK27LGWqjKLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["augmented_train_dataset_path = dest_base_path + 'train_augmented/'\n","augmented_validation_dataset_path = dest_base_path + 'validation_augmented/'\n","\n","os.makedirs(os.path.join(augmented_train_dataset_path, 'images'), exist_ok=True)\n","os.makedirs(os.path.join(augmented_validation_dataset_path, 'images'), exist_ok=True)\n","\n","if os.listdir(augmented_train_dataset_path + 'images/') == []:\n","    n_images, n_annotations = augment_dataset(train_dataset_path, augmented_train_dataset_path, get_transform('train'), n_images, n_annotations, num_augmentations=5)\n","else:\n","    print(\"Augmentation on the training set has already been made.\")\n","\n","if os.listdir(augmented_validation_dataset_path + 'images/') == []:\n","    augment_dataset(validation_dataset_path, augmented_validation_dataset_path, get_transform('validation'), n_images, n_annotations, num_augmentations=5)\n","else:\n","    print(\"Augmentation on the validation set has already been made.\")"],"metadata":{"id":"x5hw6XMkjNcr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count1 = sum(1 for filename in os.listdir(os.path.join(augmented_train_dataset_path, 'images')) if any(filename.lower().endswith(ext) for ext in ['.jpg', '.jpeg']))\n","count2 = sum(1 for filename in os.listdir(os.path.join(augmented_validation_dataset_path, 'images')) if any(filename.lower().endswith(ext) for ext in ['.jpg', '.jpeg']))\n","\n","print(f\"Number of images in the train_augmented folder: {count1}\") # TODO: make prettier\n","print(f\"Number of images in the validation_augmented folder: {count2}\") # TODO: make prettier"],"metadata":{"id":"S74KmVaTjQbA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Rewrite Datasets"],"metadata":{"id":"rBtOfa3KjPiF"}},{"cell_type":"code","source":["# TODO: add if condition (if augmentation has been executed)"],"metadata":{"id":"V4_pnILVjV1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shutil.rmtree(\"/tmp/od_data/augmented_train\") # TODO: do I need this instruction ?\n","shutil.rmtree(\"/tmp/od_data/augmented_validation\") # TODO: do I need this instruction ?"],"metadata":{"id":"UiizB7LYjmKV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = object_detector.Dataset.from_coco_folder(augmented_train_dataset_path, cache_dir=\"/tmp/od_data/augmented_train\")\n","validation_data = object_detector.Dataset.from_coco_folder(augmented_validation_dataset_path, cache_dir=\"/tmp/od_data/augmented_validation\")\n","\n","print(f\"{'New Training Dataset Size:':<25} {train_data.size:>6} images\")\n","print(f\"{'New Validation Dataset Size:':<25} {validation_data.size:>4} images\")"],"metadata":{"id":"3qLB56ehjWqa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Retrain model"],"metadata":{"id":"wnmSHDLDjmY-"}},{"cell_type":"markdown","source":["### Set retraining options"],"metadata":{"id":"ucPD2_AwjpES"}},{"cell_type":"code","source":["spec = object_detector.SupportedModels.MOBILENET_MULTI_AVG_I384\n","\n","hparams = object_detector.HParams(\n","    learning_rate=0.01, # 0.015 (is it possible to implement a scheduler?)\n","    batch_size=64, # try 128, 256\n","    epochs=100,\n","    cosine_decay_epochs=100,\n","    cosine_decay_alpha=0.1,\n","    shuffle=True, # TODO: check\n","    export_dir='exported_model'\n",")\n","\n","model_options = object_detector.ModelOptions(\n","    l2_weight_decay=1e-4 # 3e-5\n",")\n","\n","options = object_detector.ObjectDetectorOptions(\n","    supported_model=spec,\n","    hparams=hparams,\n","    model_options=model_options\n",")"],"metadata":{"id":"AK3T45OSjroj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run retraining"],"metadata":{"id":"vInD4iVejr6Q"}},{"cell_type":"code","source":["model = object_detector.ObjectDetector.create(\n","    train_data=train_data,\n","    validation_data=validation_data,\n","    options=options\n",")"],"metadata":{"id":"_nV7YsmVjuBQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluate the model performance"],"metadata":{"id":"1LrtoRJnjuX2"}},{"cell_type":"code","source":["# TODO: think about saving metrics permanently"],"metadata":{"id":"uSqMCIMDjiOQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss, coco_metrics = model.evaluate(validation_data, batch_size=32) # TODO: check batch_size\n","print(f\"Validation loss: {loss}\")\n","print(f\"Validation coco metrics: {coco_metrics}\")"],"metadata":{"id":"yaVcz_EwjxWE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Visualize metrics\n","\n","# Assuming coco_metrics is a dictionary as shown in the output above\n","\n","# 1. Graph of the main COCO metrics\n","plt.figure(figsize=(10, 6))\n","metrics = ['AP', 'AP50', 'AP75', 'APl', 'ARmax1', 'ARmax10', 'ARmax100']\n","values = [coco_metrics[m] for m in metrics]\n","plt.bar(metrics, values)\n","plt.title('COCO Metrics')\n","plt.ylabel('Score')\n","plt.ylim(0, 1)\n","for i, v in enumerate(values):\n","    plt.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n","plt.show()\n","\n","# 2. Loss distribution\n","plt.figure(figsize=(10, 6))\n","loss_types = ['total_loss', 'cls_loss', 'box_loss', 'model_loss']\n","plt.bar(loss_types, loss)\n","plt.title('Loss Distribution')\n","plt.ylabel('Loss Value')\n","for i, v in enumerate(loss):\n","    plt.text(i, v, f'{v:.4f}', ha='center', va='bottom')\n","plt.show()\n","\n","# 3. AP comparison by object size\n","plt.figure(figsize=(10, 6))\n","ap_sizes = ['APl', 'APm', 'APs']\n","ap_values = [coco_metrics[size] for size in ap_sizes]\n","plt.bar(ap_sizes, ap_values)\n","plt.title('AP by Object Size')\n","plt.ylabel('Average Precision')\n","plt.ylim(-1, 1)\n","for i, v in enumerate(ap_values):\n","    plt.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n","plt.show()"],"metadata":{"cellView":"form","id":"9DgOca-6jkWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss, coco_metrics = model.evaluate(validation_data, batch_size=64) # TODO: check batch_size\n","print(f\"Validation loss: {loss}\")\n","print(f\"Validation coco metrics: {coco_metrics}\")"],"metadata":{"id":"IU1aWbTvjqvE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Visualize metrics\n","\n","# Assuming coco_metrics is a dictionary as shown in the output above\n","\n","# 1. Graph of the main COCO metrics\n","plt.figure(figsize=(10, 6))\n","metrics = ['AP', 'AP50', 'AP75', 'APl', 'ARmax1', 'ARmax10', 'ARmax100']\n","values = [coco_metrics[m] for m in metrics]\n","plt.bar(metrics, values)\n","plt.title('COCO Metrics')\n","plt.ylabel('Score')\n","plt.ylim(0, 1)\n","for i, v in enumerate(values):\n","    plt.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n","plt.show()\n","\n","# 2. Loss distribution\n","plt.figure(figsize=(10, 6))\n","loss_types = ['total_loss', 'cls_loss', 'box_loss', 'model_loss']\n","plt.bar(loss_types, loss)\n","plt.title('Loss Distribution')\n","plt.ylabel('Loss Value')\n","for i, v in enumerate(loss):\n","    plt.text(i, v, f'{v:.4f}', ha='center', va='bottom')\n","plt.show()\n","\n","# 3. AP comparison by object size\n","plt.figure(figsize=(10, 6))\n","ap_sizes = ['APl', 'APm', 'APs']\n","ap_values = [coco_metrics[size] for size in ap_sizes]\n","plt.bar(ap_sizes, ap_values)\n","plt.title('AP by Object Size')\n","plt.ylabel('Average Precision')\n","plt.ylim(-1, 1)\n","for i, v in enumerate(ap_values):\n","    plt.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n","plt.show()"],"metadata":{"cellView":"form","id":"hXi6e7XAjuaI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Export model"],"metadata":{"id":"au58LkQIjxqz"}},{"cell_type":"code","source":["# TODO: do I need to remove the existing model first?"],"metadata":{"id":"IVgCpFLuj1NU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.export_model()\n","!ls exported_model\n","files.download('exported_model/model.tflite')"],"metadata":{"id":"a3LlhseNjyPc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model quantization"],"metadata":{"id":"oJwBCpwgj1h4"}},{"cell_type":"markdown","source":["### Quantization aware training (int8 quantization)"],"metadata":{"id":"ZZnH7GOxj2pf"}},{"cell_type":"code","source":[],"metadata":{"id":"8yS3M_t7j4Jl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Post-training quantization (fp16 quantization)"],"metadata":{"id":"t-2Ei9i7j_cA"}},{"cell_type":"code","source":["from mediapipe_model_maker import quantization"],"metadata":{"id":"TSHVUhNBkAVG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["quantization_config = quantization.QuantizationConfig.for_float16()"],"metadata":{"id":"mmqvEBh-kDtu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.restore_float_ckpt()\n","model.export_model(model_name=\"model_fp16.tflite\", quantization_config=quantization_config)\n","!ls -lh exported_model\n","files.download('exported_model/model_fp16.tflite')"],"metadata":{"id":"s-lRMhUPkFeN"},"execution_count":null,"outputs":[]}]}